{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfca857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"__af_object_type__\": \"jsonable\", \"__class__\": \"WorkflowExecutionInfo\", \"__module__\": \"ai_flow.plugin_interface.scheduler_interface\", \"_context\": null, \"_end_date\": \"0\", \"_properties\": {}, \"_start_date\": \"0\", \"_status\": \"INIT\", \"_workflow_execution_id\": \"7\", \"_workflow_info\": {\"__af_object_type__\": \"jsonable\", \"__class__\": \"WorkflowInfo\", \"__module__\": \"ai_flow.plugin_interface.scheduler_interface\", \"_namespace\": \"color_project\", \"_properties\": {}, \"_scheduling_rules\": null, \"_workflow_name\": \"color_new\"}}\n"
     ]
    }
   ],
   "source": [
    "import ai_flow as af\n",
    "from ai_flow.api.ai_flow_context import init_notebook_context\n",
    "from ai_flow_plugins.job_plugins import flink\n",
    "\n",
    "from color_processor.push_processor import ModelPushProcessor\n",
    "from color_processor.sample_processor import SampleProcessor, RawInputReader, QueueSinkProcessor, \\\n",
    "    FileSinkProcessor, DataStreamEnv, UserProfileReader, UserClickReader\n",
    "from color_processor.train_procssor import BatchTrainDataReader, BatchTrainProcessor, StreamTrainProcessor, \\\n",
    "    TrainFlinkEnv\n",
    "from color_processor.validate_processor import BatchValidateProcessor, StreamValidateProcessor, ValidateDataReader\n",
    "from recommendation import config\n",
    "from ai_flow.project.project_config import ProjectConfig\n",
    "from ai_flow.workflow.workflow_config import WorkflowConfig\n",
    "from ai_flow.workflow.job_config import JobConfig\n",
    "\n",
    "\n",
    "def workflow():\n",
    "    project_config:ProjectConfig = ProjectConfig()\n",
    "    project_config.set_project_name('color_project')\n",
    "    project_config.set_server_uri('localhost:50051')\n",
    "    project_config.set_notification_server_uri('localhost:50052')\n",
    "    project_config['blob'] = {'blob_manager_class': 'ai_flow_plugins.blob_manager_plugins.local_blob_manager.LocalBlobManager'}\n",
    "    workflow_config: WorkflowConfig = WorkflowConfig('color_new')\n",
    "    workflow_config.add_job_config('data_process', JobConfig(job_name='data_process', job_type='flink', properties={'run_mode': 'local'}))\n",
    "    workflow_config.add_job_config('batch_train', JobConfig(job_name='batch_train', job_type='flink', properties={'run_mode': 'local'}))\n",
    "    workflow_config.add_job_config('stream_train', JobConfig(job_name='stream_train', job_type='flink', properties={'run_mode': 'local'}))\n",
    "    workflow_config.add_job_config('batch_validate', JobConfig(job_name='batch_validate', job_type='python'))\n",
    "    workflow_config.add_job_config('stream_validate', JobConfig(job_name='stream_validate', job_type='python'))\n",
    "    workflow_config.add_job_config('model_push', JobConfig(job_name='model_push', job_type='python'))\n",
    "    af.init_notebook_context(project_config, workflow_config)\n",
    "\n",
    "    af.register_dataset(name=config.RawQueueDataset, uri=\"{},{}\".format(config.KafkaConn, config.RawQueueName))\n",
    "    af.register_dataset(name=config.SampleFileDataset, uri=config.SampleFileDir)\n",
    "    af.register_dataset(name=config.SampleQueueDataset, uri=\"{},{}\".format(config.KafkaConn, config.SampleQueueName))\n",
    "    af.register_dataset(name=config.ValidateDataset, uri=config.ValidateFileDir)\n",
    "    af.register_dataset(name=config.UserProfileDataset, uri=config.UserProfileTableName)\n",
    "    af.register_dataset(name=config.UserClickSnapshotDataset, uri=config.UserClickSnapshotTableName)\n",
    "    af.register_dataset(name=config.UserClickDataset, uri=config.UserClickTableName)\n",
    "\n",
    "    af.register_metric_meta(metric_name=config.BatchACC,\n",
    "                            metric_type=af.MetricType.MODEL,\n",
    "                            project_name=af.current_project_config().get_project_name())\n",
    "\n",
    "    af.register_metric_meta(metric_name=config.StreamACC,\n",
    "                            metric_type=af.MetricType.MODEL,\n",
    "                            project_name=af.current_project_config().get_project_name())\n",
    "\n",
    "    af.register_model(model_name=config.BatchModelName)\n",
    "    af.register_model(model_name=config.StreamModelName)\n",
    "    \n",
    "    with af.job_config(job_name='data_process'):\n",
    "        flink.set_flink_env(DataStreamEnv())\n",
    "        raw_input = af.read_dataset(dataset_info=config.RawQueueDataset, read_dataset_processor=RawInputReader())\n",
    "        user_profile = af.read_dataset(dataset_info=config.UserProfileDataset,\n",
    "                                       read_dataset_processor=UserProfileReader())\n",
    "        user_click = af.read_dataset(dataset_info=config.UserClickDataset, read_dataset_processor=UserClickReader())\n",
    "        validate_sample, sample = \\\n",
    "            af.user_define_operation(input=[raw_input, user_profile, user_click], processor=SampleProcessor(), output_num=2)\n",
    "\n",
    "        af.write_dataset(input=sample, dataset_info=config.SampleQueueDataset,\n",
    "                         write_dataset_processor=QueueSinkProcessor())\n",
    "        af.write_dataset(input=sample, dataset_info=config.SampleFileDataset, write_dataset_processor=FileSinkProcessor())\n",
    "        af.write_dataset(input=validate_sample, dataset_info=config.ValidateDataset, write_dataset_processor=FileSinkProcessor())\n",
    "\n",
    "    with af.job_config(job_name='batch_train'):\n",
    "        flink.set_flink_env(TrainFlinkEnv())\n",
    "        sample = af.read_dataset(config.SampleFileDataset, read_dataset_processor=BatchTrainDataReader())\n",
    "        af.train(sample, model_info=config.BatchModelName, training_processor=BatchTrainProcessor(200))\n",
    "\n",
    "    with af.job_config(job_name='batch_validate'):\n",
    "        sample = af.read_dataset(dataset_info=config.ValidateDataset, read_dataset_processor=ValidateDataReader())\n",
    "        af.user_define_operation(input=sample, processor=BatchValidateProcessor())\n",
    "\n",
    "    with af.job_config(job_name='stream_train'):\n",
    "        flink.set_flink_env(TrainFlinkEnv())\n",
    "        sample = af.read_dataset(config.SampleQueueDataset, read_dataset_processor=BatchTrainDataReader())\n",
    "        af.train(sample, model_info=config.StreamModelName, base_model_info=config.BatchModelName,\n",
    "                 training_processor=StreamTrainProcessor())\n",
    "\n",
    "    with af.job_config(job_name='stream_validate'):\n",
    "        sample = af.read_dataset(dataset_info=config.SampleQueueDataset, read_dataset_processor=ValidateDataReader())\n",
    "        af.user_define_operation(input=sample, processor=StreamValidateProcessor())\n",
    "\n",
    "    with af.job_config(job_name='model_push'):\n",
    "        af.push_model(model_info=config.StreamModelName, pushing_model_processor=ModelPushProcessor())\n",
    "\n",
    "    af.action_on_job_status(\"batch_validate\", \"batch_train\")\n",
    "\n",
    "    af.action_on_model_version_event(\"stream_train\", config.BatchModelName, 'MODEL_VALIDATED')\n",
    "    af.action_on_event(\"stream_train\", config.BatchModelName, \"*\", event_type='MODEL_VALIDATED',\n",
    "                       sender=\"batch_validate\", action=af.JobAction.NONE)\n",
    "\n",
    "    af.action_on_model_version_event(\"stream_validate\", config.StreamModelName, 'MODEL_GENERATED')\n",
    "    af.action_on_event(\"stream_validate\", config.StreamModelName, \"*\", event_type='MODEL_GENERATED',\n",
    "                       sender=\"stream_train\", action=af.JobAction.NONE)\n",
    "\n",
    "    af.action_on_model_version_event(\"model_push\", config.StreamModelName, 'MODEL_VALIDATED')\n",
    "    af.action_on_event(\"model_push\", config.StreamModelName, \"*\", event_type='MODEL_VALIDATED',\n",
    "                       sender=\"stream_validate\", action=af.JobAction.NONE)\n",
    "\n",
    "    # Run workflow\n",
    "    af.workflow_operation.stop_all_workflow_executions(af.current_workflow_config().workflow_name)\n",
    "    af.workflow_operation.submit_workflow(af.current_workflow_config().workflow_name)\n",
    "    workflow_execution = af.workflow_operation.start_new_workflow_execution(af.current_workflow_config().workflow_name)\n",
    "    print(workflow_execution)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b0a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
